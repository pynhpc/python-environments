{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Python on HPC Clusters About the Tutorials This tutorial series covers topics on using Python on HPC clusters. The specific steps are based on the HOPPER cluster at George Mason University in Fairfax, VA. They should be implementable on most HPC clusters that have the SLURM scheduler installed, the Environment Modules system for managing packages and Open onDemand for a web-based GUI to access the cluster resources. Tutorial Pages Name Description Running python Natively with a Module Covers the steps on how to run python with SLURM using python modules on HPC clusters. Running python with python virtual environments Covers managing python packages and running python scripts on HPC clusters with python virtual environments. Running python with conda-environments Covers managing python packages and running python scripts on HPC clusters with conda environments. Running python from a Notebook Covers managing python packages and running python on HPC clusters from a GUI while using either conda environments or python virtual environments. Example with Pytorch A hands-on demo with a pytorch example. Tutorial videos and Slides Links to recorded sessions and slides will be added ater the live tutorial session. Upcoming Sessions Title Date For more tutorials and other demos for the GMU Hopper Cluster, you can check out the GMU ORC Wiki pages .","title":"Python on HPC Clusters"},{"location":"#python-on-hpc-clusters","text":"","title":"Python on HPC Clusters"},{"location":"#about-the-tutorials","text":"This tutorial series covers topics on using Python on HPC clusters. The specific steps are based on the HOPPER cluster at George Mason University in Fairfax, VA. They should be implementable on most HPC clusters that have the SLURM scheduler installed, the Environment Modules system for managing packages and Open onDemand for a web-based GUI to access the cluster resources.","title":"About the Tutorials"},{"location":"#tutorial-pages","text":"Name Description Running python Natively with a Module Covers the steps on how to run python with SLURM using python modules on HPC clusters. Running python with python virtual environments Covers managing python packages and running python scripts on HPC clusters with python virtual environments. Running python with conda-environments Covers managing python packages and running python scripts on HPC clusters with conda environments. Running python from a Notebook Covers managing python packages and running python on HPC clusters from a GUI while using either conda environments or python virtual environments. Example with Pytorch A hands-on demo with a pytorch example.","title":"Tutorial Pages"},{"location":"#tutorial-videos-and-slides","text":"Links to recorded sessions and slides will be added ater the live tutorial session.","title":"Tutorial videos and Slides"},{"location":"#upcoming-sessions","text":"Title Date For more tutorials and other demos for the GMU Hopper Cluster, you can check out the GMU ORC Wiki pages .","title":"Upcoming Sessions"},{"location":"about/","text":"Python on HPC Clusters About the Tutorials This tutorial series covers topics on using Python on HPC clusters. The specific steps are based on the HOPPER cluster at George Mason University in Fairfax, VA. They should be implementable on most HPC clusters that have the SLURM scheduler installed, the Environment Modules system for managing packages and Open onDemand for a web-based GUI to access the cluster resources. Tutorial Pages Name Description Running python Natively with a Module Covers the steps on how to run python with SLURM using python modules on HPC clusters. Running python with python virtual environments Covers managing python packages and running python scripts on HPC clusters with python virtual environments. Running python with conda-environments Covers managing python packages and running python scripts on HPC clusters with conda environments. Running python from a Notebook Covers managing python packages and running python on HPC clusters from a GUI while using either conda environments or python virtual environments. Example with Pytorch A hands-on demo with a pytorch example. Tutorial videos and Slides Links to recorded sessions and slides will be added ater the live tutorial session. Upcoming Sessions Title Date For more tutorials and other demos for the GMU Hopper Cluster, you can check out the GMU ORC Wiki pages .","title":"Home"},{"location":"about/#python-on-hpc-clusters","text":"","title":"Python on HPC Clusters"},{"location":"about/#about-the-tutorials","text":"This tutorial series covers topics on using Python on HPC clusters. The specific steps are based on the HOPPER cluster at George Mason University in Fairfax, VA. They should be implementable on most HPC clusters that have the SLURM scheduler installed, the Environment Modules system for managing packages and Open onDemand for a web-based GUI to access the cluster resources.","title":"About the Tutorials"},{"location":"about/#tutorial-pages","text":"Name Description Running python Natively with a Module Covers the steps on how to run python with SLURM using python modules on HPC clusters. Running python with python virtual environments Covers managing python packages and running python scripts on HPC clusters with python virtual environments. Running python with conda-environments Covers managing python packages and running python scripts on HPC clusters with conda environments. Running python from a Notebook Covers managing python packages and running python on HPC clusters from a GUI while using either conda environments or python virtual environments. Example with Pytorch A hands-on demo with a pytorch example.","title":"Tutorial Pages"},{"location":"about/#tutorial-videos-and-slides","text":"Links to recorded sessions and slides will be added ater the live tutorial session.","title":"Tutorial videos and Slides"},{"location":"about/#upcoming-sessions","text":"Title Date For more tutorials and other demos for the GMU Hopper Cluster, you can check out the GMU ORC Wiki pages .","title":"Upcoming Sessions"},{"location":"conda-environments/","text":"Managing Python Packages with Conda Environments on the Cluster Conda environments are an excellent method for managing python packages and libraries in the cluster environment. One approach on clusters is using a centralized Anaconda3 Module, but from experience this usually caused path issues and prevented other modules from working correctly. Instead, it is possible to use a mini version of Anaconda, miniconda which includes just conda and its dependencies. It is also very small and can be downloaded directly to your /home directory. The following instructions have steps for idownloading and installing Miniconda in your home directory on the cluster and running it. Installing Maker in a python virtual environment with Miniconda3 Download miniconda3: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh Install it in your preferred path: bash miniconda.sh -b -p $HOME/miniconda You can now create a custom conda environment. In the steps below, a conda environment for the package 'Maker' is created from a conda environment file. Activate base conda environment: source $HOME/miniconda/bin/activate export PYTHONNOUSERSITE=true Download and create your virtual environment for maker using the environment yml file. This was generated from tests on the cluster so it should have all the necessary libraries. To download the environment.yml file maker_3.01.03_environment.yml use: wget https://wiki.orc.gmu.edu/mkdocs/maker/maker_3.01.03_environment.yml Then create the virtual environemnt with: conda env create -f maker_3.01.03_environment.yml You should now be able to activate it with: conda activate maker_3.01.03 Alternatively, you can simply create the environment with conda env create -n maker_env then activate it and install necessary libraries/use python with conda activate maker_3.01.03 python Running in batch mode with SLURM Below is an example of a SLURM script ( run.slurm ) to run in this conda environment. You can modify the different SLURM parameters to match what you need: Download the run.slurm script: wget https://wiki.orc.gmu.edu/mkdocs/maker/run.slurm #!/bin/bash #SBATCH --job-name=maker_test #SBATCH --partition=normal #SBATCH --nodes=1 #SBATCH --ntasks-per-node=64 #SBATCH --output=maker_test_%j.out #SBATCH --error=maker_test_%j.err #SBATCH --mem=50GB #SBATCH --time=0-12:00:00 ### Load modules module unload openmpi4 module load gnu10 openmpi ### Activate virtual environment source /home/$USER/miniconda/bin/activate source activate maker_3.01.03 ### Set environment variables: export LIBDIR=/home/$USER/miniconda/envs/maker_3.01.03/share/RepeatMasker/Libraries ### Execute program mpiexec -n ${SLURM_NTASKS_PER_NODE} maker [OPTIONS] Run this with the sbatch command: sbatch run.slurm If you ssh onto the node on which the jobs starts, you should see it now utilizes all the available cpus for maker. To see which nodes the job is running on, you can use the squeue command: squeue --me Examples: You can also install the theme by copying all of the theme files[^structure] into your project. To do so fork the Minimal Mistakes theme , then rename the repo to USERNAME.github.io --- replacing USERNAME with your GitHub username. GitHub Pages Alternatives: Looking to host your site for free and install/update the theme painlessly? [Netlify][netlify-jekyll], [GitLab Pages][gitlab-jekyll], and [Continuous Integration (CI) services][ci-jekyll] have you covered. In most cases all you need to do is connect your repository to them, create a simple configuration file, and install the theme following the Ruby Gem Method above. {: .notice--info} Remove the Unnecessary If you forked or downloaded the minimal-mistakes-jekyll repo you can safely remove the following folders and files: .editorconfig .gitattributes .github /docs /test CHANGELOG.md minimal-mistakes-jekyll.gemspec README.md screenshot-layouts.png screenshot.png Note: If forking the theme be sure to update Gemfile as well. The one found at the root of the project is for building the theme's Ruby gem and is missing dependencies. To properly setup a Gemfile with the theme, consult the \" Install Dependencies \" section. {: .notice--warning} Setup Your Site Depending on the path you took installing Minimal Mistakes you'll setup things a little differently. ProTip: The source code and content files for this site can be found in the /docs folder if you want to copy or learn from them. {: .notice--info} Then run bundle update and add theme: minimal-mistakes-jekyll to your _config.yml . v4 Breaking Change: Paths for image headers, overlays, teasers, galleries , and feature rows have changed and now require a full path. Instead of just image: filename.jpg you'll need to use the full path eg: image: /assets/images/filename.jpg . The preferred location is now /assets/images/ but can be placed elsewhere or externally hosted. This applies to image references in _config.yml and author.yml as well. {: .notice--danger} That's it! If all goes well running bundle exec jekyll serve should spin-up your site.","title":"Running in Conda Environments"},{"location":"conda-environments/#managing-python-packages-with-conda-environments-on-the-cluster","text":"Conda environments are an excellent method for managing python packages and libraries in the cluster environment. One approach on clusters is using a centralized Anaconda3 Module, but from experience this usually caused path issues and prevented other modules from working correctly. Instead, it is possible to use a mini version of Anaconda, miniconda which includes just conda and its dependencies. It is also very small and can be downloaded directly to your /home directory. The following instructions have steps for idownloading and installing Miniconda in your home directory on the cluster and running it.","title":"Managing Python Packages with Conda Environments on the Cluster"},{"location":"conda-environments/#installing-maker-in-a-python-virtual-environment-with-miniconda3","text":"Download miniconda3: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh Install it in your preferred path: bash miniconda.sh -b -p $HOME/miniconda You can now create a custom conda environment. In the steps below, a conda environment for the package 'Maker' is created from a conda environment file. Activate base conda environment: source $HOME/miniconda/bin/activate export PYTHONNOUSERSITE=true Download and create your virtual environment for maker using the environment yml file. This was generated from tests on the cluster so it should have all the necessary libraries. To download the environment.yml file maker_3.01.03_environment.yml use: wget https://wiki.orc.gmu.edu/mkdocs/maker/maker_3.01.03_environment.yml Then create the virtual environemnt with: conda env create -f maker_3.01.03_environment.yml You should now be able to activate it with: conda activate maker_3.01.03 Alternatively, you can simply create the environment with conda env create -n maker_env then activate it and install necessary libraries/use python with conda activate maker_3.01.03 python","title":"Installing Maker in a python virtual environment with Miniconda3"},{"location":"conda-environments/#running-in-batch-mode-with-slurm","text":"Below is an example of a SLURM script ( run.slurm ) to run in this conda environment. You can modify the different SLURM parameters to match what you need: Download the run.slurm script: wget https://wiki.orc.gmu.edu/mkdocs/maker/run.slurm #!/bin/bash #SBATCH --job-name=maker_test #SBATCH --partition=normal #SBATCH --nodes=1 #SBATCH --ntasks-per-node=64 #SBATCH --output=maker_test_%j.out #SBATCH --error=maker_test_%j.err #SBATCH --mem=50GB #SBATCH --time=0-12:00:00 ### Load modules module unload openmpi4 module load gnu10 openmpi ### Activate virtual environment source /home/$USER/miniconda/bin/activate source activate maker_3.01.03 ### Set environment variables: export LIBDIR=/home/$USER/miniconda/envs/maker_3.01.03/share/RepeatMasker/Libraries ### Execute program mpiexec -n ${SLURM_NTASKS_PER_NODE} maker [OPTIONS] Run this with the sbatch command: sbatch run.slurm If you ssh onto the node on which the jobs starts, you should see it now utilizes all the available cpus for maker. To see which nodes the job is running on, you can use the squeue command: squeue --me","title":"Running in batch mode with SLURM"},{"location":"conda-environments/#examples","text":"You can also install the theme by copying all of the theme files[^structure] into your project. To do so fork the Minimal Mistakes theme , then rename the repo to USERNAME.github.io --- replacing USERNAME with your GitHub username. GitHub Pages Alternatives: Looking to host your site for free and install/update the theme painlessly? [Netlify][netlify-jekyll], [GitLab Pages][gitlab-jekyll], and [Continuous Integration (CI) services][ci-jekyll] have you covered. In most cases all you need to do is connect your repository to them, create a simple configuration file, and install the theme following the Ruby Gem Method above. {: .notice--info}","title":"Examples:"},{"location":"conda-environments/#remove-the-unnecessary","text":"If you forked or downloaded the minimal-mistakes-jekyll repo you can safely remove the following folders and files: .editorconfig .gitattributes .github /docs /test CHANGELOG.md minimal-mistakes-jekyll.gemspec README.md screenshot-layouts.png screenshot.png Note: If forking the theme be sure to update Gemfile as well. The one found at the root of the project is for building the theme's Ruby gem and is missing dependencies. To properly setup a Gemfile with the theme, consult the \" Install Dependencies \" section. {: .notice--warning}","title":"Remove the Unnecessary"},{"location":"conda-environments/#setup-your-site","text":"Depending on the path you took installing Minimal Mistakes you'll setup things a little differently. ProTip: The source code and content files for this site can be found in the /docs folder if you want to copy or learn from them. {: .notice--info} Then run bundle update and add theme: minimal-mistakes-jekyll to your _config.yml . v4 Breaking Change: Paths for image headers, overlays, teasers, galleries , and feature rows have changed and now require a full path. Instead of just image: filename.jpg you'll need to use the full path eg: image: /assets/images/filename.jpg . The preferred location is now /assets/images/ but can be placed elsewhere or externally hosted. This applies to image references in _config.yml and author.yml as well. {: .notice--danger} That's it! If all goes well running bundle exec jekyll serve should spin-up your site.","title":"Setup Your Site"},{"location":"example-with-pytorch/","text":"Running Pytorch Jobs on Hopper Runs requiring Pytorch can be run directly using the system installed python or a python virtual environment or any one of the available containers. Running with the System Python in Batch Mode To run with the system python, log in to the cluster AMD head node which has a gpu card that allows for testing gpu codes. ssh NetID@hopper-amd.orc.gmu.edu On the hopper-amd headnode, load the GNU 10 and default python - version 3.9.9 module load gnu10 module load python You can test and work on short running scripts directly on the headnode: python main.py Once you're ready, use a SLURM script to run your code in batch mode. When setting the SLURM parameters in your script, pay attention to the the following: 1 - The partition and QOS: #SBATCH --partition=gpuq #SBATCH --qos=gpuq Groups that have access to the gpuq can use the contrib-gpuq partition with the corresponding group QOS. The time limit on the gpuq partiton defaults to 3 days: #SBATCH --time=3-00:00:00 but can be set to a maximum of 5 days. 2 - GPU node options: Type of GPU SLURM setting No. of GPUs on Node No. of CPUs RAM A100 80GB --gres=gpu:A100.80gb:nGPUS 4 64 500GB DGX A100 40GB --gres=gpu:A100.40gb:nGPUs 8 128 1TB Below is a sample SLURM submission script. Save the information into run.slurm , update the timing information, the <N_CPU_CORES> , <MEMORY> and <N_GPUs> to reflect the number of CPU cores and GPUs you need (referring to the table above) and submit it by entering sbatch run.slurm Sample script, run.slurm : #!/bin/bash #SBATCH --partition=gpuq #SBATCH --qos=gpu #SBATCH --job-name=gpu_basics #SBATCH --output=gpu_basics.%j.out #SBATCH --error=gpu_basics.%j.out #SBATCH --nodes=1 #SBATCH --ntasks-per-node=<N_CPU_CORES> #SBATCH --gres=gpu:A100.80gb:<N_GPUs> #SBATCH --mem=<MEMORY> #SBATCH --export=ALL #SBATCH -time=0-01:00:00 # set echo umask 0022 # to see ID and state of GPUs assigned nvidia-smi ## Load the necessary modules module load gnu10 module load python ## Execute your script python main.py Running interactively on a GPU node To work directly on a gpu node, start interactive session with salloc -p gpuq -q gpu --nodes=1 --ntasks-per-node=4 --constraint=amd --gres=gpu:A100.80gb:1 --mem=50GB --pty $SHELL The gres and other parameters can be adjusted to match the required resources. Once on the gpu node, the same steps as outlined above can be followed to set up your environment and run your code. If no time limit had been set the session will continue until it is ended with exit This will take you back to the head node. Managing pytorch and other packages with python virtual environments To use additional libraries or different versions of pytorch and other libraries than what is available in the python modules, use Python Virtual Environments . The following steps summarize the process. NOTE : To make sure your Python Virtual Environment runs across all nodes, it is important to use the versions of modules built for this. Before creating the Python Virtual Environment 1 - First switch modules to GNU 10 compilations: module load gnu10/10.3.0-ya 2 - Check and load python module module avail python module load python/3.8.6-pi 3 - Now you can create the python virtual environment. After it is created, activate it and upgrade pip python -m vevn pytorch-env source pytorch-env/bin/activate python -m pip install --upgrade pip 5 - Remove system python module and install torch modules (Refer to this page for updated instructions on installing Pytorch) module unload python pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html 6 - Install additional modules as needed, test your scripts and deactivate the python virtual environment once you're done. In your SLURM script, to run with the Python Virtual Environment, you activate the Python Virtual Environment instead of loading the python module. Sample script, run.slurm , for python virtual environments: #!/bin/bash #SBATCH --partition=gpuq #SBATCH --qos=gpu #SBATCH --job-name=gpu_basics #SBATCH --output=gpu_basics.%j.out #SBATCH --error=gpu_basics.%j.out #SBATCH --nodes=1 #SBATCH --ntasks-per-node=<N_CPU_CORES> #SBATCH --gres=gpu:A100.80gb:<N_GPUs> #SBATCH --mem=<MEMORY> #SBATCH --export=ALL #SBATCH -time=0-01:00:00 # set echo umask 0022 # to see ID and state of GPUs assigned nvidia-smi ## Load the necessary modules module load gnu10 source activate ~/pytorch-env/bin/activate ## Execute your script python main.py Running Pytorch with Singularity Containers Containers and examples for running Pytorch are available for all users can be found on the cluster at /containers/dgx/Containers/pytorch and /containers/dgx/Examples/Pytorch You can copy the set up to your working directory on Hopper directly by cp -r /containers/dgx/Examples/Pytorch/misc/* . You can then modify the scripts to run in your directories. In the available example scripts the environmental variable $SINGULARITY_BASE points to /containers/dgx/Containers . Check these pages for examples on runnig containerized version of Pytorch. Common Pytorch Issues GPU not recognized - In your Python Virtual Environment, you probably need to use an earlier version of Pytorch. Replace your pytorch install by re-installing an earlier verison e.g.: pip3 install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html CUDA Memory Error - Refer to this page for documentation on dealing with this error: Pytorch FAQs","title":"Example with Pytorch"},{"location":"example-with-pytorch/#running-pytorch-jobs-on-hopper","text":"Runs requiring Pytorch can be run directly using the system installed python or a python virtual environment or any one of the available containers.","title":"Running Pytorch Jobs on Hopper"},{"location":"example-with-pytorch/#running-with-the-system-python-in-batch-mode","text":"To run with the system python, log in to the cluster AMD head node which has a gpu card that allows for testing gpu codes. ssh NetID@hopper-amd.orc.gmu.edu On the hopper-amd headnode, load the GNU 10 and default python - version 3.9.9 module load gnu10 module load python You can test and work on short running scripts directly on the headnode: python main.py Once you're ready, use a SLURM script to run your code in batch mode. When setting the SLURM parameters in your script, pay attention to the the following: 1 - The partition and QOS: #SBATCH --partition=gpuq #SBATCH --qos=gpuq Groups that have access to the gpuq can use the contrib-gpuq partition with the corresponding group QOS. The time limit on the gpuq partiton defaults to 3 days: #SBATCH --time=3-00:00:00 but can be set to a maximum of 5 days. 2 - GPU node options: Type of GPU SLURM setting No. of GPUs on Node No. of CPUs RAM A100 80GB --gres=gpu:A100.80gb:nGPUS 4 64 500GB DGX A100 40GB --gres=gpu:A100.40gb:nGPUs 8 128 1TB Below is a sample SLURM submission script. Save the information into run.slurm , update the timing information, the <N_CPU_CORES> , <MEMORY> and <N_GPUs> to reflect the number of CPU cores and GPUs you need (referring to the table above) and submit it by entering sbatch run.slurm Sample script, run.slurm : #!/bin/bash #SBATCH --partition=gpuq #SBATCH --qos=gpu #SBATCH --job-name=gpu_basics #SBATCH --output=gpu_basics.%j.out #SBATCH --error=gpu_basics.%j.out #SBATCH --nodes=1 #SBATCH --ntasks-per-node=<N_CPU_CORES> #SBATCH --gres=gpu:A100.80gb:<N_GPUs> #SBATCH --mem=<MEMORY> #SBATCH --export=ALL #SBATCH -time=0-01:00:00 # set echo umask 0022 # to see ID and state of GPUs assigned nvidia-smi ## Load the necessary modules module load gnu10 module load python ## Execute your script python main.py","title":"Running with the System Python in Batch Mode"},{"location":"example-with-pytorch/#running-interactively-on-a-gpu-node","text":"To work directly on a gpu node, start interactive session with salloc -p gpuq -q gpu --nodes=1 --ntasks-per-node=4 --constraint=amd --gres=gpu:A100.80gb:1 --mem=50GB --pty $SHELL The gres and other parameters can be adjusted to match the required resources. Once on the gpu node, the same steps as outlined above can be followed to set up your environment and run your code. If no time limit had been set the session will continue until it is ended with exit This will take you back to the head node.","title":"Running interactively on a GPU node"},{"location":"example-with-pytorch/#managing-pytorch-and-other-packages-with-python-virtual-environments","text":"To use additional libraries or different versions of pytorch and other libraries than what is available in the python modules, use Python Virtual Environments . The following steps summarize the process. NOTE : To make sure your Python Virtual Environment runs across all nodes, it is important to use the versions of modules built for this. Before creating the Python Virtual Environment 1 - First switch modules to GNU 10 compilations: module load gnu10/10.3.0-ya 2 - Check and load python module module avail python module load python/3.8.6-pi 3 - Now you can create the python virtual environment. After it is created, activate it and upgrade pip python -m vevn pytorch-env source pytorch-env/bin/activate python -m pip install --upgrade pip 5 - Remove system python module and install torch modules (Refer to this page for updated instructions on installing Pytorch) module unload python pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html 6 - Install additional modules as needed, test your scripts and deactivate the python virtual environment once you're done. In your SLURM script, to run with the Python Virtual Environment, you activate the Python Virtual Environment instead of loading the python module. Sample script, run.slurm , for python virtual environments: #!/bin/bash #SBATCH --partition=gpuq #SBATCH --qos=gpu #SBATCH --job-name=gpu_basics #SBATCH --output=gpu_basics.%j.out #SBATCH --error=gpu_basics.%j.out #SBATCH --nodes=1 #SBATCH --ntasks-per-node=<N_CPU_CORES> #SBATCH --gres=gpu:A100.80gb:<N_GPUs> #SBATCH --mem=<MEMORY> #SBATCH --export=ALL #SBATCH -time=0-01:00:00 # set echo umask 0022 # to see ID and state of GPUs assigned nvidia-smi ## Load the necessary modules module load gnu10 source activate ~/pytorch-env/bin/activate ## Execute your script python main.py","title":"Managing pytorch and other packages with python virtual environments"},{"location":"example-with-pytorch/#running-pytorch-with-singularity-containers","text":"Containers and examples for running Pytorch are available for all users can be found on the cluster at /containers/dgx/Containers/pytorch and /containers/dgx/Examples/Pytorch You can copy the set up to your working directory on Hopper directly by cp -r /containers/dgx/Examples/Pytorch/misc/* . You can then modify the scripts to run in your directories. In the available example scripts the environmental variable $SINGULARITY_BASE points to /containers/dgx/Containers . Check these pages for examples on runnig containerized version of Pytorch.","title":"Running Pytorch with Singularity Containers"},{"location":"example-with-pytorch/#common-pytorch-issues","text":"GPU not recognized - In your Python Virtual Environment, you probably need to use an earlier version of Pytorch. Replace your pytorch install by re-installing an earlier verison e.g.: pip3 install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html CUDA Memory Error - Refer to this page for documentation on dealing with this error: Pytorch FAQs","title":"Common Pytorch Issues"},{"location":"jupyter-notebooks/","text":"Running Jupyter Notebooks on the ORC Clusters JupyterLab is available as one of the interactive apps that can be launched directly from Open OnDemand. To use Jupyter Notebooks on Hopper, the first step is to start an Open OnDemand (OOD)session. In your browser, go to ondemand.orc.gmu.edu . You will need to authenticate with your Mason credentials before you can access the dashboard. Once you authenticate, you should be directed to the OOD dashboard. {: style=\"height:300px\"} You'll need to click on the \"Interactive Apps\" icon to see the apps that are available. From the OOD Dashboard Select JupyterLab from the listed apps {: style=\"height:550px\"} Set your confiuration from the given options and launch. {: style=\"height:550px\"} Once your session is ready to start, you can 'Connect' {: style=\"height:550px\"} This will start a Jupyter Lab session where you can select to start Python Notebook. From the OOD Hopper Desktop App The other way to run Jupyter Notebooks on the OOD server is by starting a desktop session. * Start by selecting the Hopper Desktop option from the interactive apps. {: style=\"height:450px\"} Click on the Hopper Desktop and set your configuration (number of hours upto 12 and number of cores upto 12). You'll see the launch option once the resources become available. {: style=\"height:550px\"} After launching the session, you'll get connected to a node on Hopper with a desktop environment from where you can now start the terminal (using the highlighted 'terminal emulator' icon) {: style=\"height:585px\"} From the launched shell session, load the python module with module load python and start a Jupyter Notebook with jupyter-notebook {: style=\"height:510px\"} The Jupyter Notebook session should now start. Running Longer Python Jobs The Jupyter Notebok sessions are interactive and can be run for a maximum of 12 hours at a time. To run longer python jobs, you need to convert your code into a coherent python script that can then be submitted through SLURM. Instructions on running conventional Python jobs are given in these pages","title":"Running from Jupyter Notebooks"},{"location":"jupyter-notebooks/#running-jupyter-notebooks-on-the-orc-clusters","text":"JupyterLab is available as one of the interactive apps that can be launched directly from Open OnDemand. To use Jupyter Notebooks on Hopper, the first step is to start an Open OnDemand (OOD)session. In your browser, go to ondemand.orc.gmu.edu . You will need to authenticate with your Mason credentials before you can access the dashboard. Once you authenticate, you should be directed to the OOD dashboard. {: style=\"height:300px\"} You'll need to click on the \"Interactive Apps\" icon to see the apps that are available.","title":"Running Jupyter Notebooks on the ORC Clusters"},{"location":"jupyter-notebooks/#from-the-ood-dashboard","text":"Select JupyterLab from the listed apps {: style=\"height:550px\"} Set your confiuration from the given options and launch. {: style=\"height:550px\"} Once your session is ready to start, you can 'Connect' {: style=\"height:550px\"} This will start a Jupyter Lab session where you can select to start Python Notebook.","title":"From the OOD Dashboard"},{"location":"jupyter-notebooks/#from-the-ood-hopper-desktop-app","text":"The other way to run Jupyter Notebooks on the OOD server is by starting a desktop session. * Start by selecting the Hopper Desktop option from the interactive apps. {: style=\"height:450px\"} Click on the Hopper Desktop and set your configuration (number of hours upto 12 and number of cores upto 12). You'll see the launch option once the resources become available. {: style=\"height:550px\"} After launching the session, you'll get connected to a node on Hopper with a desktop environment from where you can now start the terminal (using the highlighted 'terminal emulator' icon) {: style=\"height:585px\"} From the launched shell session, load the python module with module load python and start a Jupyter Notebook with jupyter-notebook {: style=\"height:510px\"} The Jupyter Notebook session should now start.","title":"From the OOD Hopper Desktop App"},{"location":"jupyter-notebooks/#running-longer-python-jobs","text":"The Jupyter Notebok sessions are interactive and can be run for a maximum of 12 hours at a time. To run longer python jobs, you need to convert your code into a coherent python script that can then be submitted through SLURM. Instructions on running conventional Python jobs are given in these pages","title":"Running Longer Python Jobs"},{"location":"python-on-hpc/","text":"Running Python on the ORC Clusters [!NOTE] To run Python jobs or build Python Virtual Environments that will run across all the nodes on Hopper, use the Python modules under the GCC/10 compiler: ``` module load gnu10 module load python/<version> ## the default python is version 3.10 ``` The examples below will be based on the Hopper cluster . Slight modifications in the SLURM scripts will be necessary to run them on Argo. Python Versions To see the available version of Python, run the command ml spider python This will list all the available versions of Python that are installed on the cluster and include all the different builds. ------------------------------------------------------------------------------------------------ python: ------------------------------------------------------------------------------------------------ Versions: python/2.7.18-z2 python/2.7.18-z4 python/3.7.4-rg python/3.7.6-iu python/3.7.6-ks python/3.7.7-intel python/3.8.6-ff python/3.8.6-pi python/3.8.6-kg python/3.8.6-rp python/3.8.6-vw python/3.8.6-ye python/3.8.6-p2 python/3.8.6-4q python/3.9.7-intel python/3.9.9-jh Other possible modules matches: intel-python intelpython ------------------------------------------------------------------------------------------------ To find other possible module matches execute: $ module -r spider '.*python.*' ------------------------------------------------------------------------------------------------ You can also run module load gnu10 module avail python Which will show you only the GCC builds or the Intel builds, depending on which compiler you're working with. Going with the recommended option (GNU-10.3.0 build), you should see -------------------------------------------- GNU-10.3.0 --------------------------------------------- python/3.8.6-pi python/3.9.9-jh (D) Where: D: Default Module Running module load python will load the default version. You can also load a specific version with module load python/<version> With the Python module now in your path, you should be able to execute Python commands and run Python scripts. Running a Python Job Interactively on a CPU Python jobs should not be run directly on the head nodes . The preferred method, even if you're testing a small job, is to start a debug session directly on a compute node using the debug partition and then test your script or, for short jobs, run it directly from the node. The default time limit on the debug partition is 1 hour. To get more information on the available partitions, resources, and limits on the node use the sinfo command. To connect directly to a compute node and use the debug partition, use the salloc command together with additional SLURM parameters salloc -p debug -n 1 --cpus-per-task=12 --mem=15GB However, if you want to run an interactive job that may require a time limit of more than 1 hour use the command shown below : salloc -p normal -n 1 --cpus-per-task=12 --mem=15GB -t 0-02:00:00 This command will allocate you a single node with 12 cores and 15GB of memory for 2 hours on the normal partition. Once the resources become available, your prompt should now show that you're on one of the Hopper nodes. salloc: Granted job allocation salloc: Waiting for resource configuration salloc: Nodes hop065 are ready for job [user@hop065 ~]$ Modules you loaded while on the head nodes are exported onto the node as well. If you had not already loaded any modules, you should be able to load them now as well. To check the currently loaded modules on the node use the command shown below : [user@hop065 ~]$ module list Currently Loaded Modules: 1) use.own 3) prun/2.0 5) gnu10/10.3.0-ya 7) sqlite/3.37.1-6s 9) openmpi/4.1.2-4a 2) autotools 4) hosts/hopper 6) zlib/1.2.11-2y 8) tcl/8.6.11-d4 10) python/3.9.9-jh Inactive Modules: 1) openmpi4 You can now start Python to debug your code or use it interactively. [user@hop065 ~]$ python Python 3.9.9 (main, Mar 25 2022, 16:08:31) [GCC 10.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> You could also run your Python script directly $ python myscript.py The interactive session will persist until you type the 'exit' command as shown below: $ exit exit salloc: Relinquishing job allocation Interactively on a GPU In a similar manner, you can start an interactive session on a GPU node with salloc -p gpuq -q gpu --ntasks-per-node=1 --gres=gpu:A100.40gb:1 -t 0-01:00:00 Using a SLURM Submission Script Once your tests are done and you're ready to run longer Python jobs, you should now switch to using the batch submission with SLURM. To do this, you write a SLURM script setting the different parameters for your job, loading the necessary modules, and executing your Python script which is then submitted to the selected queue from where it will run your job. Below is an example SLURM script ( run.slurm ): #!/bin/bash #SBATCH --partition=normal # will run on any cpus in the 'normal' partition #SBATCH --job-name=python-cpu ## Separate output and error messages into 2 files. ## NOTE: %u=userID, %x=jobName, %N=nodeID, %j=jobID #SBATCH --output=/scratch/%u/%x-%N-%j.out # Output file #SBATCH --error=/scratch/%u/%x-%N-%j.err # Error file #SBATCH --nodes=1 #SBATCH --cpus-per-task=1 # up to 48 per node #SBATCH --mem-per-cpu=3500M # memory per CORE; maximum is 180GB per node #SBATCH --export=ALL #SBATCH --time=0-01:00:00 # set to 1hr; please choose carefully set echo umask 0027 module load gnu10 module load python # load the recommended Python version python myscript.py # execute your Python script If you need GPU nodes for your Python job, you would change the partition information to the gpuq and set the number of GPU nodes needed. #!/bin/bash #SBATCH --partition=gpuq # the DGX only belongs in the 'gpu' partition #SBATCH --qos=gpu # need to select 'gpu' QoS #SBATCH --job-name=python-gpu ## Separate output and error messages into 2 files. ## NOTE: %u=userID, %x=jobName, %N=nodeID, %j=jobID #SBATCH --output=/scratch/%u/%x-%N-%j.out # Output file #SBATCH --error=/scratch/%u/%x-%N-%j.err # Error file #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 # up to 128; #SBATCH --gres=gpu:A100.80gb:1 # up to 8; only request what you need #SBATCH --mem-per-cpu=3500M # memory per CORE; total memory is 1 TB (1,000,000 MB) #SBATCH --export=ALL #SBATCH --time=0-01:00:00 # set to 1hr; please choose carefully set echo umask 0027 # to see ID and state of GPUs assigned nvidia-smi module load gnu10 module load python python myscript.py Please use the scratch space to submit your job's SLURM script with sbatch run.slurm To access scratch space use cd /scratch/UserID command to change directories(replace 'UserId' with your GMU GMUnetID). Please note that scratch directories have no space limit and data in /scratch gets purged 90 days from the date of creation, so make sure to move your files to a safe place before the purge. To copy files directly from scratch to your project space you can use the cp command to create a copy of the contents of the file or directory specified by the SourceFile or SourceDirectory parameters into the file or directory specified by the TargetFile or TargetDirectory parameters. The cp command also copies entire directories into other directories if you specify the -r or -R flags. The command below copies entire files from the scratch space to your project space (\" /projects/orctest\" as shown in the example below, where \" /projects/orctest\" is a project space) [UserId@hopper2 ~]$ cd /scratch/UserId [UserId@hopper2 UserId]$ cp -p -r * /projects/orctest Optimizing your GPU runs Current available GPU node options Type of GPU SLURM setting No. of GPUs on Node No. of CPUs RAM A100 80GB --gres=gpu:A100.80gb:nGPUS 4 64 500GB DGX A100 40GB --gres=gpu:A100.40gb:nGPUs 8 128 1TB The way the GPU nodes are partitioned will likely change over time to optimize utilization. The best way to take advantage of this Multi-Instance GPU (MIG) mode operation is to analyze the demands of your job and determine which GPU slice is available and suitable for it. For example, if your simulation uses very small GPU memory, you would be better off using a 1g.5gb (where 5GB is the memory you get in the GPU) slice and leaving the bigger slices to jobs that need more GPU memory. Another consideration for machine learning jobs is the difference in demands of training and inference tasks. Training tasks require more computation and memory, therefore they perform best on a full GPU node or a large slice, whereas inference tasks can be adequately performed on smaller slices. You would modify your SLURM script so that you are now requesting a suitable GPU slice: #!/bin/bash #SBATCH --partition=gpuq # the DGX only belongs in the 'gpu' partition #SBATCH --qos=gpu # need to select 'gpu' QoS #SBATCH --job-name=python-gpu ## Separate output and error messages into 2 files. ## NOTE: %u=userID, %x=jobName, %N=nodeID, %j=jobID #SBATCH --output=/scratch/%u/%x-%N-%j.out # Output file #SBATCH --error=/scratch/%u/%x-%N-%j.err # Error file #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 # up to 128; #SBATCH --gres=gpu:1g.10gb:1 # request a slice of the GPU #SBATCH --mem-per-cpu=3500M # memory per CORE; total memory is 1 TB (1,000,000 MB) #SBATCH --export=ALL #SBATCH --time=0-01:00:00 # set to 1hr; please choose carefully set echo umask 0027 # to see ID and state of GPUs assigned nvidia-smi module load gnu10 module load python python myscript.py Read more about the Hopper GPU nodes and other examples on the DGX USER GUIDE. Running Parallel Python Jobs When working on a cluster computer, it is natural to ask how to take advantage of all these nodes and cores in order to speed up computation as much as possible. On a laptop, one common approach is to use the Pool class in the Python multiprocessing library to distribute computation to other cores on the machine. While this approach certainly works on a cluster too, it does not allow you to take full advantage of the available computing power. Each job is limited to a single node and all the cores that are currently available on it. Multithreaded Python Job Below is an example SLURM script that can be used to run a Python script that implements the 'multiprocessing' module. #!/bin/sh ## Give your job a name to distinguish it from other jobs you run. #SBATCH --job-name=threaded #SBATCH --partition=normal # will run on any cpus in the 'normal' partition ## Separate output and error messages into 2 files. ## NOTE: %u=userID, %x=jobName, %N=nodeID, %j=jobID #SBATCH --output=/scratch/%u/%x-%N-%j.out # Output file #SBATCH --error=/scratch/%u/%x-%N-%j.err # Error file #SBATCH --constraint=amd ## or intel #SBATCH --nodes=1 ## all threads need to be on a single node #SBATCH --cpus-per-task=24 ## 48 or 64 processor #SBATCH --mem=5G # Total memory needed per task (units: K,M,G,T) #SBATCH --time=0-01:00:00 # set to 1hr; please choose carefully set echo umask 0027 # to see ID and state of GPUs assigned nvidia-smi ## Load the relevant modules needed for the job module load gnu10 module load python ## Run your program or script python <your-threaded-script>.py The Python script below which can be downloaded here: multithreaded.py can be used as a test case for threaded jobs in Python. #!/usr/bin/env python import numpy as np import multiprocessing as mp if __name__ == '__main__': np.random.seed(0); # create two matrices to be passed # to two different processes mat1 = np.random.rand(3,3); mat2 = np.random.rand(2,2); # define number of processes ntasks =2; # create a pool of processes p = mp.Pool(ntasks); # feed different process with same task # but different data and print the result print(p.map(np.linalg.eigvals,[mat1,mat2])) Distributed Python Jobs with mpi4py The mpi4py library has a Pool-like class that is very similar to the one in the multiprocessing library. Here, we describe how to setup a Python virtual environment to use mpi4py run Python code to take advantage of a much larger number of cores. Installing mpi4py in a Python Virtual Environment When installing Python modules, we recommend using a Python Virtual Environment . When working on a project you may want to install a number of different packages. We recommend creating one VE for each project and installing everything that you need into it. For the purposes of this demonstration, let\u2019s create a virtual environment called MPIpool and install mpi4py into it. [UserId@hopper1 ~]$ module load gnu10 [UserId@hopper1 ~]$ module load python [UserId@hopper1 ~]$ module load openmpi4/4.1.2 [UserId@hopper1 ~]$ python -m venv ~/MPIpool [UserId@hopper1 ~]$ source ~/MPIpool/bin/activate (MPIpool) [UserId@hopper1 ~]$ pip install mpi4py Collecting mpi4py Using cached h\u2063ttps://files.pythonhosted.org/packages/04/f5/a615603ce4ab7f40b65dba63759455e3da610d9a155d4d4cece1d8fd6706/mpi4py-3.0.2.tar.gz Installing collected packages: mpi4py Running setup.py install for mpi4py ... done Successfully installed mpi4py-3.0.2 Using MPIPoolExecutor in a Python Program Here we have a sample Python program (which can be downloaded here: MPIpool.py ) that calculates prime numbers. It uses the MPIPoolExecutor class to farm out calculations to \"workers\". The workers can be running on any node and core in the cluster. There must always be one \"manager\" that is responsible for farming out the work, and collecting the results when finished. # MPIpool.py from mpi4py.futures import MPIPoolExecutor import math import textwrap def calc_primes(range_tuple): \"\"\"Calculate all the prime numbers in the given range.\"\"\" low, high = range_tuple if low <= 2 < high: primes = [2] else: primes = [] start = max(3,low) # Don't start below 3 if start % 2 == 0: # Make sure start is odd, i.e. skip evens start += 1 for num in range(start, high, 2): # increment by 2's, i.e. skip evens if all(num % i != 0 for i in range(3, int(math.sqrt(num)) + 1, 2)): primes.append(num) return primes def determine_subranges(fullrange, num_subranges): \"\"\" Break fullrange up into smaller sets of ranges that cover all the same numbers. \"\"\" subranges = [] inc = fullrange[1] // num_subranges for i in range(fullrange[0], fullrange[1], inc): subranges.append( (i, min(i+inc, fullrange[1])) ) return( subranges ) if __name__ == '__main__': fullrange = (0, 100000000) num_subranges = 1000 subranges = determine_subranges(fullrange, num_subranges) executor = MPIPoolExecutor() prime_sets = executor.map(calc_primes, subranges) executor.shutdown() # flatten the list of lists primes = [p for plist in prime_sets for p in plist] print(textwrap.fill(str(primes),80)) The main work is done in the calc_primes() function, which is what the workers run. It calculates all the prime numbers within a range defined by rangeTuple , a vector that contains two values: the lower and upper bounds of the range. The rest of the code runs on the \"manager\". It calls the determine_subranges() function to define the different pieces of work to send to the workers. The MPIPoolExecutor.map() function actually handles all the complexity of coordinating communications with workers, farming out the different tasks, and then collecting the results. The mpi4py documentation suggest that when using MPIPoolExecutor , your code should use the if __name__ == '__main__': code construct at the bottom of your main file in order to prevent workers from spawning more workers . Submitting the Program to SLURM Here we provide a SLURM script for running such a job. #!/bin/sh ## Give your job a name to distinguish it from other jobs you run. #SBATCH --job-name=MPIpool #SBATCH --partition=normal ## Separate output and error messages into 2 files. ## NOTE: %u=userID, %x=jobName, %N=nodeID, %j=jobID, %A=arrayID, %a=arrayTaskID #SBATCH --output=/scratch/%u/%x-%N-%j.out # Output file #SBATCH --error=/scratch/%u/%x-%N-%j.err # Error file ## Slurm can send you updates via email #SBATCH --mail-type=BEGIN,END,FAIL # ALL,NONE,BEGIN,END,FAIL,REQUEUE,.. #SBATCH --mail-user=<GMUnetID>@gmu.edu # Put your GMU email address here ## Specify how much memory your job needs. (2G is the default) #SBATCH --mem=8G # Total memory needed per task (units: K,M,G,T) ## Specify how much time your job needs. (default: see partition above) #SBATCH --time=0-02:00 # Total time needed for job: Days-Hours:Minutes #SBATCH --ntasks=51 # 50 workers, 1 manager set echo umask 0027 # to see ID and state of GPUs assigned nvidia-smi ## Load the relevant modules needed for the job module load gnu10 module load python module load openmpi4/4.1.2 source ~/MPIpool/bin/activate ## Run your program or script mpirun -np $SLURM_NTASKS python -m mpi4py.futures MPIpool.py Be sure to replace the (including the < and > ) with you own email address. Note that we set --ntasks=51 in order to allocate 1 manager and 50 workers. There must always be only 1 manager and at least 1 worker. Note that we use the $SLURM_NTASKS environment variable in the call to mpirun to make sure that the number of cores used equals the number allocated by the --ntasks= option. Because mpi4py is based on the MPI libraries, we need to load one of the MPI modules. Here we have chosen OpenMPI. The mpirun or mpiexec program must be used to properly launch an MPI program, and this program is no exception. The runtime for this program using 50 workers is about 1 minute. That is significantly faster than the 45 minutes needed to run the program using a single core. Of course, there is a point of diminishing returns (and even an added cost) in adding more and more workers. It is good to experiment with different numbers to see how many workers are optimal. The maximum number of cores that a user can request is currently 300. This may change in the future. This is an example of an algorithm that is \"embarrassingly parallel\". It is very easy to divide it up into smaller pieces and pass them out. Many algorithms are not so easy to parallelize in this way. MPI is a very mature library, and it has the tools to handle problems that are much more complex than this. It is the de facto standard for doing large scale parallelization, and if that is your goal you can benefit from learning more about it. Those interested in a more \"Pythonic\" library may want to look into Dask . Using External Python Packages To install and run your Python code with external Python packages, after loading the Python module, first create a directory for storing those packages (e.g. ~/python-packages/projectX ) mkdir ~/python-packages mkdir ~/python-packages/projectX Then install the appropriate packages in there: pip install <package1> -t ~/python-packages/projectX To run your code with these extra packages, you would need to add the export command to your SLURM submission script so that the last part would now be module load gnu10 module load python export PYTHONPATH=~/python-packages/projectX:$PYTHONPATH python myscript.py If instead, you were running interactively, then you need to run the export command from the terminal $ export PYTHONPATH=~/python-packages/projectX:$PYTHONPATH Running with Python Virtual Environments To have better control over the Python packages and libraries you need to use on the cluster, the best way to run Python is using Python Virtual Environments. This is especially useful for codes that use Tensorflow, Keras or Pytorch. Read our instructions on building Python Virtual Environments and how to run Tensorflow . Remember When running on Argo, the SLURM scripts have to be updated so that they can be run on Argo. The main differences between Argo and Hopper are detailed in these pages . Running with Jupyter NoteBooks You also have the option of using Jupyter Notebooks (on Hopper) to run Python code. The steps for doing this are outlined in these pages.","title":"Running Python as a Module"},{"location":"python-on-hpc/#running-python-on-the-orc-clusters","text":"[!NOTE] To run Python jobs or build Python Virtual Environments that will run across all the nodes on Hopper, use the Python modules under the GCC/10 compiler: ``` module load gnu10 module load python/<version> ## the default python is version 3.10 ``` The examples below will be based on the Hopper cluster . Slight modifications in the SLURM scripts will be necessary to run them on Argo.","title":"Running Python on the ORC Clusters"},{"location":"python-on-hpc/#python-versions","text":"To see the available version of Python, run the command ml spider python This will list all the available versions of Python that are installed on the cluster and include all the different builds. ------------------------------------------------------------------------------------------------ python: ------------------------------------------------------------------------------------------------ Versions: python/2.7.18-z2 python/2.7.18-z4 python/3.7.4-rg python/3.7.6-iu python/3.7.6-ks python/3.7.7-intel python/3.8.6-ff python/3.8.6-pi python/3.8.6-kg python/3.8.6-rp python/3.8.6-vw python/3.8.6-ye python/3.8.6-p2 python/3.8.6-4q python/3.9.7-intel python/3.9.9-jh Other possible modules matches: intel-python intelpython ------------------------------------------------------------------------------------------------ To find other possible module matches execute: $ module -r spider '.*python.*' ------------------------------------------------------------------------------------------------ You can also run module load gnu10 module avail python Which will show you only the GCC builds or the Intel builds, depending on which compiler you're working with. Going with the recommended option (GNU-10.3.0 build), you should see -------------------------------------------- GNU-10.3.0 --------------------------------------------- python/3.8.6-pi python/3.9.9-jh (D) Where: D: Default Module Running module load python will load the default version. You can also load a specific version with module load python/<version> With the Python module now in your path, you should be able to execute Python commands and run Python scripts.","title":"Python Versions"},{"location":"python-on-hpc/#running-a-python-job","text":"","title":"Running a Python Job"},{"location":"python-on-hpc/#interactively-on-a-cpu","text":"Python jobs should not be run directly on the head nodes . The preferred method, even if you're testing a small job, is to start a debug session directly on a compute node using the debug partition and then test your script or, for short jobs, run it directly from the node. The default time limit on the debug partition is 1 hour. To get more information on the available partitions, resources, and limits on the node use the sinfo command. To connect directly to a compute node and use the debug partition, use the salloc command together with additional SLURM parameters salloc -p debug -n 1 --cpus-per-task=12 --mem=15GB However, if you want to run an interactive job that may require a time limit of more than 1 hour use the command shown below : salloc -p normal -n 1 --cpus-per-task=12 --mem=15GB -t 0-02:00:00 This command will allocate you a single node with 12 cores and 15GB of memory for 2 hours on the normal partition. Once the resources become available, your prompt should now show that you're on one of the Hopper nodes. salloc: Granted job allocation salloc: Waiting for resource configuration salloc: Nodes hop065 are ready for job [user@hop065 ~]$ Modules you loaded while on the head nodes are exported onto the node as well. If you had not already loaded any modules, you should be able to load them now as well. To check the currently loaded modules on the node use the command shown below : [user@hop065 ~]$ module list Currently Loaded Modules: 1) use.own 3) prun/2.0 5) gnu10/10.3.0-ya 7) sqlite/3.37.1-6s 9) openmpi/4.1.2-4a 2) autotools 4) hosts/hopper 6) zlib/1.2.11-2y 8) tcl/8.6.11-d4 10) python/3.9.9-jh Inactive Modules: 1) openmpi4 You can now start Python to debug your code or use it interactively. [user@hop065 ~]$ python Python 3.9.9 (main, Mar 25 2022, 16:08:31) [GCC 10.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> You could also run your Python script directly $ python myscript.py The interactive session will persist until you type the 'exit' command as shown below: $ exit exit salloc: Relinquishing job allocation","title":"Interactively on a CPU"},{"location":"python-on-hpc/#interactively-on-a-gpu","text":"In a similar manner, you can start an interactive session on a GPU node with salloc -p gpuq -q gpu --ntasks-per-node=1 --gres=gpu:A100.40gb:1 -t 0-01:00:00","title":"Interactively on a GPU"},{"location":"python-on-hpc/#using-a-slurm-submission-script","text":"Once your tests are done and you're ready to run longer Python jobs, you should now switch to using the batch submission with SLURM. To do this, you write a SLURM script setting the different parameters for your job, loading the necessary modules, and executing your Python script which is then submitted to the selected queue from where it will run your job. Below is an example SLURM script ( run.slurm ): #!/bin/bash #SBATCH --partition=normal # will run on any cpus in the 'normal' partition #SBATCH --job-name=python-cpu ## Separate output and error messages into 2 files. ## NOTE: %u=userID, %x=jobName, %N=nodeID, %j=jobID #SBATCH --output=/scratch/%u/%x-%N-%j.out # Output file #SBATCH --error=/scratch/%u/%x-%N-%j.err # Error file #SBATCH --nodes=1 #SBATCH --cpus-per-task=1 # up to 48 per node #SBATCH --mem-per-cpu=3500M # memory per CORE; maximum is 180GB per node #SBATCH --export=ALL #SBATCH --time=0-01:00:00 # set to 1hr; please choose carefully set echo umask 0027 module load gnu10 module load python # load the recommended Python version python myscript.py # execute your Python script If you need GPU nodes for your Python job, you would change the partition information to the gpuq and set the number of GPU nodes needed. #!/bin/bash #SBATCH --partition=gpuq # the DGX only belongs in the 'gpu' partition #SBATCH --qos=gpu # need to select 'gpu' QoS #SBATCH --job-name=python-gpu ## Separate output and error messages into 2 files. ## NOTE: %u=userID, %x=jobName, %N=nodeID, %j=jobID #SBATCH --output=/scratch/%u/%x-%N-%j.out # Output file #SBATCH --error=/scratch/%u/%x-%N-%j.err # Error file #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 # up to 128; #SBATCH --gres=gpu:A100.80gb:1 # up to 8; only request what you need #SBATCH --mem-per-cpu=3500M # memory per CORE; total memory is 1 TB (1,000,000 MB) #SBATCH --export=ALL #SBATCH --time=0-01:00:00 # set to 1hr; please choose carefully set echo umask 0027 # to see ID and state of GPUs assigned nvidia-smi module load gnu10 module load python python myscript.py Please use the scratch space to submit your job's SLURM script with sbatch run.slurm To access scratch space use cd /scratch/UserID command to change directories(replace 'UserId' with your GMU GMUnetID). Please note that scratch directories have no space limit and data in /scratch gets purged 90 days from the date of creation, so make sure to move your files to a safe place before the purge. To copy files directly from scratch to your project space you can use the cp command to create a copy of the contents of the file or directory specified by the SourceFile or SourceDirectory parameters into the file or directory specified by the TargetFile or TargetDirectory parameters. The cp command also copies entire directories into other directories if you specify the -r or -R flags. The command below copies entire files from the scratch space to your project space (\" /projects/orctest\" as shown in the example below, where \" /projects/orctest\" is a project space) [UserId@hopper2 ~]$ cd /scratch/UserId [UserId@hopper2 UserId]$ cp -p -r * /projects/orctest","title":"Using a SLURM Submission Script"},{"location":"python-on-hpc/#optimizing-your-gpu-runs","text":"Current available GPU node options Type of GPU SLURM setting No. of GPUs on Node No. of CPUs RAM A100 80GB --gres=gpu:A100.80gb:nGPUS 4 64 500GB DGX A100 40GB --gres=gpu:A100.40gb:nGPUs 8 128 1TB The way the GPU nodes are partitioned will likely change over time to optimize utilization. The best way to take advantage of this Multi-Instance GPU (MIG) mode operation is to analyze the demands of your job and determine which GPU slice is available and suitable for it. For example, if your simulation uses very small GPU memory, you would be better off using a 1g.5gb (where 5GB is the memory you get in the GPU) slice and leaving the bigger slices to jobs that need more GPU memory. Another consideration for machine learning jobs is the difference in demands of training and inference tasks. Training tasks require more computation and memory, therefore they perform best on a full GPU node or a large slice, whereas inference tasks can be adequately performed on smaller slices. You would modify your SLURM script so that you are now requesting a suitable GPU slice: #!/bin/bash #SBATCH --partition=gpuq # the DGX only belongs in the 'gpu' partition #SBATCH --qos=gpu # need to select 'gpu' QoS #SBATCH --job-name=python-gpu ## Separate output and error messages into 2 files. ## NOTE: %u=userID, %x=jobName, %N=nodeID, %j=jobID #SBATCH --output=/scratch/%u/%x-%N-%j.out # Output file #SBATCH --error=/scratch/%u/%x-%N-%j.err # Error file #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 # up to 128; #SBATCH --gres=gpu:1g.10gb:1 # request a slice of the GPU #SBATCH --mem-per-cpu=3500M # memory per CORE; total memory is 1 TB (1,000,000 MB) #SBATCH --export=ALL #SBATCH --time=0-01:00:00 # set to 1hr; please choose carefully set echo umask 0027 # to see ID and state of GPUs assigned nvidia-smi module load gnu10 module load python python myscript.py Read more about the Hopper GPU nodes and other examples on the DGX USER GUIDE.","title":"Optimizing your GPU runs"},{"location":"python-on-hpc/#running-parallel-python-jobs","text":"When working on a cluster computer, it is natural to ask how to take advantage of all these nodes and cores in order to speed up computation as much as possible. On a laptop, one common approach is to use the Pool class in the Python multiprocessing library to distribute computation to other cores on the machine. While this approach certainly works on a cluster too, it does not allow you to take full advantage of the available computing power. Each job is limited to a single node and all the cores that are currently available on it.","title":"Running Parallel Python Jobs"},{"location":"python-on-hpc/#multithreaded-python-job","text":"Below is an example SLURM script that can be used to run a Python script that implements the 'multiprocessing' module. #!/bin/sh ## Give your job a name to distinguish it from other jobs you run. #SBATCH --job-name=threaded #SBATCH --partition=normal # will run on any cpus in the 'normal' partition ## Separate output and error messages into 2 files. ## NOTE: %u=userID, %x=jobName, %N=nodeID, %j=jobID #SBATCH --output=/scratch/%u/%x-%N-%j.out # Output file #SBATCH --error=/scratch/%u/%x-%N-%j.err # Error file #SBATCH --constraint=amd ## or intel #SBATCH --nodes=1 ## all threads need to be on a single node #SBATCH --cpus-per-task=24 ## 48 or 64 processor #SBATCH --mem=5G # Total memory needed per task (units: K,M,G,T) #SBATCH --time=0-01:00:00 # set to 1hr; please choose carefully set echo umask 0027 # to see ID and state of GPUs assigned nvidia-smi ## Load the relevant modules needed for the job module load gnu10 module load python ## Run your program or script python <your-threaded-script>.py The Python script below which can be downloaded here: multithreaded.py can be used as a test case for threaded jobs in Python. #!/usr/bin/env python import numpy as np import multiprocessing as mp if __name__ == '__main__': np.random.seed(0); # create two matrices to be passed # to two different processes mat1 = np.random.rand(3,3); mat2 = np.random.rand(2,2); # define number of processes ntasks =2; # create a pool of processes p = mp.Pool(ntasks); # feed different process with same task # but different data and print the result print(p.map(np.linalg.eigvals,[mat1,mat2]))","title":"Multithreaded Python Job"},{"location":"python-on-hpc/#distributed-python-jobs-with-mpi4py","text":"The mpi4py library has a Pool-like class that is very similar to the one in the multiprocessing library. Here, we describe how to setup a Python virtual environment to use mpi4py run Python code to take advantage of a much larger number of cores.","title":"Distributed Python Jobs with mpi4py"},{"location":"python-on-hpc/#installing-mpi4py-in-a-python-virtual-environment","text":"When installing Python modules, we recommend using a Python Virtual Environment . When working on a project you may want to install a number of different packages. We recommend creating one VE for each project and installing everything that you need into it. For the purposes of this demonstration, let\u2019s create a virtual environment called MPIpool and install mpi4py into it. [UserId@hopper1 ~]$ module load gnu10 [UserId@hopper1 ~]$ module load python [UserId@hopper1 ~]$ module load openmpi4/4.1.2 [UserId@hopper1 ~]$ python -m venv ~/MPIpool [UserId@hopper1 ~]$ source ~/MPIpool/bin/activate (MPIpool) [UserId@hopper1 ~]$ pip install mpi4py Collecting mpi4py Using cached h\u2063ttps://files.pythonhosted.org/packages/04/f5/a615603ce4ab7f40b65dba63759455e3da610d9a155d4d4cece1d8fd6706/mpi4py-3.0.2.tar.gz Installing collected packages: mpi4py Running setup.py install for mpi4py ... done Successfully installed mpi4py-3.0.2","title":"Installing mpi4py in a Python Virtual Environment"},{"location":"python-on-hpc/#using-mpipoolexecutor-in-a-python-program","text":"Here we have a sample Python program (which can be downloaded here: MPIpool.py ) that calculates prime numbers. It uses the MPIPoolExecutor class to farm out calculations to \"workers\". The workers can be running on any node and core in the cluster. There must always be one \"manager\" that is responsible for farming out the work, and collecting the results when finished. # MPIpool.py from mpi4py.futures import MPIPoolExecutor import math import textwrap def calc_primes(range_tuple): \"\"\"Calculate all the prime numbers in the given range.\"\"\" low, high = range_tuple if low <= 2 < high: primes = [2] else: primes = [] start = max(3,low) # Don't start below 3 if start % 2 == 0: # Make sure start is odd, i.e. skip evens start += 1 for num in range(start, high, 2): # increment by 2's, i.e. skip evens if all(num % i != 0 for i in range(3, int(math.sqrt(num)) + 1, 2)): primes.append(num) return primes def determine_subranges(fullrange, num_subranges): \"\"\" Break fullrange up into smaller sets of ranges that cover all the same numbers. \"\"\" subranges = [] inc = fullrange[1] // num_subranges for i in range(fullrange[0], fullrange[1], inc): subranges.append( (i, min(i+inc, fullrange[1])) ) return( subranges ) if __name__ == '__main__': fullrange = (0, 100000000) num_subranges = 1000 subranges = determine_subranges(fullrange, num_subranges) executor = MPIPoolExecutor() prime_sets = executor.map(calc_primes, subranges) executor.shutdown() # flatten the list of lists primes = [p for plist in prime_sets for p in plist] print(textwrap.fill(str(primes),80)) The main work is done in the calc_primes() function, which is what the workers run. It calculates all the prime numbers within a range defined by rangeTuple , a vector that contains two values: the lower and upper bounds of the range. The rest of the code runs on the \"manager\". It calls the determine_subranges() function to define the different pieces of work to send to the workers. The MPIPoolExecutor.map() function actually handles all the complexity of coordinating communications with workers, farming out the different tasks, and then collecting the results. The mpi4py documentation suggest that when using MPIPoolExecutor , your code should use the if __name__ == '__main__': code construct at the bottom of your main file in order to prevent workers from spawning more workers .","title":"Using MPIPoolExecutor in a Python Program"},{"location":"python-on-hpc/#submitting-the-program-to-slurm","text":"Here we provide a SLURM script for running such a job. #!/bin/sh ## Give your job a name to distinguish it from other jobs you run. #SBATCH --job-name=MPIpool #SBATCH --partition=normal ## Separate output and error messages into 2 files. ## NOTE: %u=userID, %x=jobName, %N=nodeID, %j=jobID, %A=arrayID, %a=arrayTaskID #SBATCH --output=/scratch/%u/%x-%N-%j.out # Output file #SBATCH --error=/scratch/%u/%x-%N-%j.err # Error file ## Slurm can send you updates via email #SBATCH --mail-type=BEGIN,END,FAIL # ALL,NONE,BEGIN,END,FAIL,REQUEUE,.. #SBATCH --mail-user=<GMUnetID>@gmu.edu # Put your GMU email address here ## Specify how much memory your job needs. (2G is the default) #SBATCH --mem=8G # Total memory needed per task (units: K,M,G,T) ## Specify how much time your job needs. (default: see partition above) #SBATCH --time=0-02:00 # Total time needed for job: Days-Hours:Minutes #SBATCH --ntasks=51 # 50 workers, 1 manager set echo umask 0027 # to see ID and state of GPUs assigned nvidia-smi ## Load the relevant modules needed for the job module load gnu10 module load python module load openmpi4/4.1.2 source ~/MPIpool/bin/activate ## Run your program or script mpirun -np $SLURM_NTASKS python -m mpi4py.futures MPIpool.py Be sure to replace the (including the < and > ) with you own email address. Note that we set --ntasks=51 in order to allocate 1 manager and 50 workers. There must always be only 1 manager and at least 1 worker. Note that we use the $SLURM_NTASKS environment variable in the call to mpirun to make sure that the number of cores used equals the number allocated by the --ntasks= option. Because mpi4py is based on the MPI libraries, we need to load one of the MPI modules. Here we have chosen OpenMPI. The mpirun or mpiexec program must be used to properly launch an MPI program, and this program is no exception. The runtime for this program using 50 workers is about 1 minute. That is significantly faster than the 45 minutes needed to run the program using a single core. Of course, there is a point of diminishing returns (and even an added cost) in adding more and more workers. It is good to experiment with different numbers to see how many workers are optimal. The maximum number of cores that a user can request is currently 300. This may change in the future. This is an example of an algorithm that is \"embarrassingly parallel\". It is very easy to divide it up into smaller pieces and pass them out. Many algorithms are not so easy to parallelize in this way. MPI is a very mature library, and it has the tools to handle problems that are much more complex than this. It is the de facto standard for doing large scale parallelization, and if that is your goal you can benefit from learning more about it. Those interested in a more \"Pythonic\" library may want to look into Dask .","title":"Submitting the Program to SLURM"},{"location":"python-on-hpc/#using-external-python-packages","text":"To install and run your Python code with external Python packages, after loading the Python module, first create a directory for storing those packages (e.g. ~/python-packages/projectX ) mkdir ~/python-packages mkdir ~/python-packages/projectX Then install the appropriate packages in there: pip install <package1> -t ~/python-packages/projectX To run your code with these extra packages, you would need to add the export command to your SLURM submission script so that the last part would now be module load gnu10 module load python export PYTHONPATH=~/python-packages/projectX:$PYTHONPATH python myscript.py If instead, you were running interactively, then you need to run the export command from the terminal $ export PYTHONPATH=~/python-packages/projectX:$PYTHONPATH","title":"Using External Python Packages"},{"location":"python-on-hpc/#running-with-python-virtual-environments","text":"To have better control over the Python packages and libraries you need to use on the cluster, the best way to run Python is using Python Virtual Environments. This is especially useful for codes that use Tensorflow, Keras or Pytorch. Read our instructions on building Python Virtual Environments and how to run Tensorflow . Remember When running on Argo, the SLURM scripts have to be updated so that they can be run on Argo. The main differences between Argo and Hopper are detailed in these pages .","title":"Running with Python Virtual Environments"},{"location":"python-on-hpc/#running-with-jupyter-notebooks","text":"You also have the option of using Jupyter Notebooks (on Hopper) to run Python code. The steps for doing this are outlined in these pages.","title":"Running with Jupyter NoteBooks"},{"location":"virtual-environments/","text":"Python Virtual Environments On Hopper There are different approaches in the Python ecosystem for creating virtual environments (VE): pyenv , venv , virtualenv , virtualenvwrapper , and pipenv . Among these, the methods venv and virtualenv are quite similar, and either one can be chosen. The key distinction between these two lies in how they handle Python executables within the virtual environment folder: venv creates a virtual environment without copying Python executables. virtualenv creates a virtual environment by copying Python executables. Before utilizing Python Virtual Environments on Hopper, please take note of the following: Python environments need to be established separately on Hopper. A Python Virtual environment built on Hopper should not be expected to function on any other system. For Hopper, it is recommended to employ venv for creating virtual environments, following the process detailed below. Starting an interactive Session on Hopper GPUs If you intend to run the Python Virtual Environments on the Hopper DGX-A100 GPU nodes, the first step is to get directly on the GPU node by starting an interactive session with the salloc command salloc -p gpuq -q gpu -n 1 --ntasks-per-node=1 --gres=gpu:A100.40gb:1 --mem=50GB Currently available GPU node options Type of GPU SLURM setting No. of GPUs on Node No. of CPUs RAM A100 80GB --gres=gpu:A100.80gb:nGPUS 4 64 500GB DGX A100 40GB --gres=gpu:A100.40gb:nGPUs 8 128 1TB If you don't want to use DGX nodes you can also use the below command to start an interactive session on the contrib-gpuq partition salloc -p contrib-gpuq -q gpu -n 1 --ntasks-per-node=1 --gres=gpu:1 --mem=5GB The way the GPU nodes are partitioned will likely change over time to optimize utilization. Use the sinfo command to view the list of nodes available, the time restriction for each node, and the available partitions. Once you have the interactive session started, you should load the necessary modules. For Python on the DGX nodes, use the module python/3.8.6-ff which has been built to run across both the CPU nodes and GPU nodes. Currently, other Python modules will not work across both the GPU and CPU nodes because of the differences in architecture. Creating a Python Virtual Environment venv is a Python module available in the standard library and compatible with most Python versions. It simplifies the creation of isolated environments for Python projects. Unlike other environment management tools, venv ensures that libraries are not shared with other virtual environments by default. You also have the option to prevent access to globally installed libraries if desired. When you utilize venv to create a virtual environment, it establishes an isolated space where you can install your project's dependencies independently. Unlike tools like Virtualenv, venv does not rely on copying the Python interpreter binary into the virtual environment directory. Instead, it often employs symbolic links to the existing Python interpreter on your system. As a result, the virtual environment created by venv contains symbolic links to the Python executable, optimizing disk space usage. To summarize, venv is a built-in module within Python's standard library that offers a simple method for establishing isolated environments tailored to Python projects. It avoids library sharing between virtual environments and enables efficient management of project-specific dependencies. Python Virtual Environment Setup for Hopper Cluster To ensure that your Python Virtual Environment runs consistently across all nodes, it's important to use modules built for this purpose. Before creating the Python Virtual Environment, follow these steps: 1 - First switch modules to GNU 10 compilations: module load gnu10/10.3.0-ya 2 - Check and load python module module avail python module load python/3.8.6-pi 3 - Now you can create the python virtual environment. After it is created, activate it and upgrade pip python -m venv py-env source py-env/bin/activate python -m pip install --upgrade pip 4 - Remove system python module and install modules module unload python pip install sklearn 5 - Install additional modules as needed, test your scripts and deactivate the python virtual environment once you're done. deactivate Example: ================================================================== [user@hopper2 ~]$ python -m venv py-env [user@hopper2 ~]$ source py-env/bin/activate (py-env) [user@hopper2 ~]$ python -m pip install --upgrade pip Collecting pip Downloading pip-23.2.1-py3-none-any.whl (2.1 MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.1 MB 13.6 MB/s Installing collected packages: pip Attempting uninstall: pip Found existing installation: pip 20.2.1 Uninstalling pip-20.2.1: Successfully uninstalled pip-20.2.1 Successfully installed pip-23.2.1 (py-env) [user@hopper2 ~]$ module unload python (py-env) [user@hopper2 ~]$ pip install sklearn Collecting sklearn Downloading sklearn-0.0.post7.tar.gz (3.6 kB) Installing build dependencies ... done Getting requirements to build wheel ... done Preparing metadata (pyproject.toml) ... done Building wheels for collected packages: sklearn Building wheel for sklearn (pyproject.toml) ... done Created wheel for sklearn: filename=sklearn-0.0.post7-py3-none-any.whl size=2951 sha256=c62d78ad5864da7dddeea4f58c96b35c6bab2584dec8748ce5687b788491f2eb Stored in directory: /home/smudund/.cache/pip/wheels/bc/86/46/dd4e366dc5e1303b4d6927d2a603a1ae7f979d488a5d202330 Successfully built sklearn Installing collected packages: sklearn Successfully installed sklearn-0.0.post7 (py-env) [user@hopper2 ~]$ deactivate [user@hopper2 ~]$ Things to remember before creating a virtual environment To create a Python Virtual Environment from a login node, a Python module must first be loaded using the \u2018module load\u2019 command . On Hopper , you need to make sure you have loaded one of the correct Python modules compiled under GNU 10 to run across all nodes. Do this by first loading the gnu10 module and then the Python modules under it: module load gnu10 module load python/<version> Using \u201cpip install \u201d within an activated Virtual Environment installs the package in the Virtual Environment\u2019s home directory and is available only to that Virtual Environment. However, if you use \u201cpip install --user \u201d within an activated Virtual Environment, then the package is installed both in the Virtual Environment\u2019s home directory and in /home/$USER/.local directory. By being installed in the /home/$USER/.local directory that package is then available to all Virtual Environments you create. So, the use of \u201c--user\u201d option with pip install is not recommended. Using a Python Virtual Environment in a SLURM submission script Below is a sample SLURM submission script. Save the information into run.slurm , update the timing information, the <N_CPU_CORES> , <MEMORY> and <N_GPUs> to reflect the number of CPU cores and GPUs you need (referring to the table above) and submit it by entering sbatch run.slurm Sample script, run.slurm : #!/bin/bash #SBATCH --partition=gpuq #SBATCH --qos=gpu #SBATCH --job-name=gpu_basics #SBATCH --output=gpu_basics.%j.out #SBATCH --error=gpu_basics.%j.out #SBATCH --nodes=1 #SBATCH --ntasks-per-node=<N_CPU_CORES> #SBATCH --gres=gpu:A100.80gb:<N_GPUs> #SBATCH --mem=<MEMORY> #SBATCH --export=ALL #SBATCH -time=0-01:00:00 # set echo umask 0022 # to see ID and state of GPUs assigned nvidia-smi ## Load the necessary modules module load gnu10 module load python ## Execute your script python main.py Adding your Python Virtual Environment as a Kernel in JupyterLabs Python Virtual Environments created on Hopper can be added as kernels to the JupyterLab sessions started under Open OnDemand . To see your Python Virtual Environment as a kernel, first, activate the virtual environment from the command line: source test-env/bin/activate Next pip install ipykernel pip install ipykernel and then add the kernel for the virtual environment Python -m ipykernel install --user --name=env-name Finally, deactivate your environment. After this, the next time you start a JupyterLab session on Open OnDemand you should see your added kernel as an option.","title":"Running in Python Virtual Environments"},{"location":"virtual-environments/#python-virtual-environments-on-hopper","text":"There are different approaches in the Python ecosystem for creating virtual environments (VE): pyenv , venv , virtualenv , virtualenvwrapper , and pipenv . Among these, the methods venv and virtualenv are quite similar, and either one can be chosen. The key distinction between these two lies in how they handle Python executables within the virtual environment folder: venv creates a virtual environment without copying Python executables. virtualenv creates a virtual environment by copying Python executables. Before utilizing Python Virtual Environments on Hopper, please take note of the following: Python environments need to be established separately on Hopper. A Python Virtual environment built on Hopper should not be expected to function on any other system. For Hopper, it is recommended to employ venv for creating virtual environments, following the process detailed below.","title":"Python Virtual Environments On Hopper"},{"location":"virtual-environments/#starting-an-interactive-session-on-hopper-gpus","text":"If you intend to run the Python Virtual Environments on the Hopper DGX-A100 GPU nodes, the first step is to get directly on the GPU node by starting an interactive session with the salloc command salloc -p gpuq -q gpu -n 1 --ntasks-per-node=1 --gres=gpu:A100.40gb:1 --mem=50GB Currently available GPU node options Type of GPU SLURM setting No. of GPUs on Node No. of CPUs RAM A100 80GB --gres=gpu:A100.80gb:nGPUS 4 64 500GB DGX A100 40GB --gres=gpu:A100.40gb:nGPUs 8 128 1TB If you don't want to use DGX nodes you can also use the below command to start an interactive session on the contrib-gpuq partition salloc -p contrib-gpuq -q gpu -n 1 --ntasks-per-node=1 --gres=gpu:1 --mem=5GB The way the GPU nodes are partitioned will likely change over time to optimize utilization. Use the sinfo command to view the list of nodes available, the time restriction for each node, and the available partitions. Once you have the interactive session started, you should load the necessary modules. For Python on the DGX nodes, use the module python/3.8.6-ff which has been built to run across both the CPU nodes and GPU nodes. Currently, other Python modules will not work across both the GPU and CPU nodes because of the differences in architecture.","title":"Starting an interactive Session on Hopper GPUs"},{"location":"virtual-environments/#creating-a-python-virtual-environment","text":"venv is a Python module available in the standard library and compatible with most Python versions. It simplifies the creation of isolated environments for Python projects. Unlike other environment management tools, venv ensures that libraries are not shared with other virtual environments by default. You also have the option to prevent access to globally installed libraries if desired. When you utilize venv to create a virtual environment, it establishes an isolated space where you can install your project's dependencies independently. Unlike tools like Virtualenv, venv does not rely on copying the Python interpreter binary into the virtual environment directory. Instead, it often employs symbolic links to the existing Python interpreter on your system. As a result, the virtual environment created by venv contains symbolic links to the Python executable, optimizing disk space usage. To summarize, venv is a built-in module within Python's standard library that offers a simple method for establishing isolated environments tailored to Python projects. It avoids library sharing between virtual environments and enables efficient management of project-specific dependencies.","title":"Creating a Python Virtual Environment"},{"location":"virtual-environments/#python-virtual-environment-setup-for-hopper-cluster","text":"To ensure that your Python Virtual Environment runs consistently across all nodes, it's important to use modules built for this purpose. Before creating the Python Virtual Environment, follow these steps: 1 - First switch modules to GNU 10 compilations: module load gnu10/10.3.0-ya 2 - Check and load python module module avail python module load python/3.8.6-pi 3 - Now you can create the python virtual environment. After it is created, activate it and upgrade pip python -m venv py-env source py-env/bin/activate python -m pip install --upgrade pip 4 - Remove system python module and install modules module unload python pip install sklearn 5 - Install additional modules as needed, test your scripts and deactivate the python virtual environment once you're done. deactivate","title":"Python Virtual Environment Setup for Hopper Cluster"},{"location":"virtual-environments/#example","text":"================================================================== [user@hopper2 ~]$ python -m venv py-env [user@hopper2 ~]$ source py-env/bin/activate (py-env) [user@hopper2 ~]$ python -m pip install --upgrade pip Collecting pip Downloading pip-23.2.1-py3-none-any.whl (2.1 MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.1 MB 13.6 MB/s Installing collected packages: pip Attempting uninstall: pip Found existing installation: pip 20.2.1 Uninstalling pip-20.2.1: Successfully uninstalled pip-20.2.1 Successfully installed pip-23.2.1 (py-env) [user@hopper2 ~]$ module unload python (py-env) [user@hopper2 ~]$ pip install sklearn Collecting sklearn Downloading sklearn-0.0.post7.tar.gz (3.6 kB) Installing build dependencies ... done Getting requirements to build wheel ... done Preparing metadata (pyproject.toml) ... done Building wheels for collected packages: sklearn Building wheel for sklearn (pyproject.toml) ... done Created wheel for sklearn: filename=sklearn-0.0.post7-py3-none-any.whl size=2951 sha256=c62d78ad5864da7dddeea4f58c96b35c6bab2584dec8748ce5687b788491f2eb Stored in directory: /home/smudund/.cache/pip/wheels/bc/86/46/dd4e366dc5e1303b4d6927d2a603a1ae7f979d488a5d202330 Successfully built sklearn Installing collected packages: sklearn Successfully installed sklearn-0.0.post7 (py-env) [user@hopper2 ~]$ deactivate [user@hopper2 ~]$","title":"Example:"},{"location":"virtual-environments/#things-to-remember-before-creating-a-virtual-environment","text":"To create a Python Virtual Environment from a login node, a Python module must first be loaded using the \u2018module load\u2019 command . On Hopper , you need to make sure you have loaded one of the correct Python modules compiled under GNU 10 to run across all nodes. Do this by first loading the gnu10 module and then the Python modules under it: module load gnu10 module load python/<version> Using \u201cpip install \u201d within an activated Virtual Environment installs the package in the Virtual Environment\u2019s home directory and is available only to that Virtual Environment. However, if you use \u201cpip install --user \u201d within an activated Virtual Environment, then the package is installed both in the Virtual Environment\u2019s home directory and in /home/$USER/.local directory. By being installed in the /home/$USER/.local directory that package is then available to all Virtual Environments you create. So, the use of \u201c--user\u201d option with pip install is not recommended.","title":"Things to remember before creating a virtual environment"},{"location":"virtual-environments/#using-a-python-virtual-environment-in-a-slurm-submission-script","text":"Below is a sample SLURM submission script. Save the information into run.slurm , update the timing information, the <N_CPU_CORES> , <MEMORY> and <N_GPUs> to reflect the number of CPU cores and GPUs you need (referring to the table above) and submit it by entering sbatch run.slurm Sample script, run.slurm : #!/bin/bash #SBATCH --partition=gpuq #SBATCH --qos=gpu #SBATCH --job-name=gpu_basics #SBATCH --output=gpu_basics.%j.out #SBATCH --error=gpu_basics.%j.out #SBATCH --nodes=1 #SBATCH --ntasks-per-node=<N_CPU_CORES> #SBATCH --gres=gpu:A100.80gb:<N_GPUs> #SBATCH --mem=<MEMORY> #SBATCH --export=ALL #SBATCH -time=0-01:00:00 # set echo umask 0022 # to see ID and state of GPUs assigned nvidia-smi ## Load the necessary modules module load gnu10 module load python ## Execute your script python main.py","title":"Using a Python Virtual Environment in a SLURM submission script"},{"location":"virtual-environments/#adding-your-python-virtual-environment-as-a-kernel-in-jupyterlabs","text":"Python Virtual Environments created on Hopper can be added as kernels to the JupyterLab sessions started under Open OnDemand . To see your Python Virtual Environment as a kernel, first, activate the virtual environment from the command line: source test-env/bin/activate Next pip install ipykernel pip install ipykernel and then add the kernel for the virtual environment Python -m ipykernel install --user --name=env-name Finally, deactivate your environment. After this, the next time you start a JupyterLab session on Open OnDemand you should see your added kernel as an option.","title":"Adding your Python Virtual Environment as a Kernel in JupyterLabs"}]}